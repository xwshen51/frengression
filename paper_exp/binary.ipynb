{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53989f4",
   "metadata": {},
   "source": [
    "## Experiments for binary treatment effect estimation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19983602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# add the project root to sys.path\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "from data_causl.utils import *\n",
    "from data_causl.data import *\n",
    "from frengression import *\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "import CausalEGM as cegm\n",
    "# import the module\n",
    "from models import *\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_tr = 1000\n",
    "n_p = 1000\n",
    "\n",
    "nI = 2\n",
    "nX = 2\n",
    "nO = 2\n",
    "nS= 2\n",
    "p = nI+nX+nO+nS\n",
    "ate = 2\n",
    "beta_cov = 0\n",
    "strength_instr = 1\n",
    "strength_conf = 1\n",
    "strength_outcome = 1\n",
    "binary_intervention=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801550b",
   "metadata": {},
   "source": [
    "## Example of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbba039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "def tune_and_eval(model_name,\n",
    "                  X_train, t_train, y_train,\n",
    "                  X_val,   t_val,   y_val,\n",
    "                  X_test,  t_test,  y_test,\n",
    "                  provided_params=None,\n",
    "                  n_trials=20):\n",
    "    \"\"\"\n",
    "    If best_params is None: runs Optuna, returns (ITE_array, best_params).\n",
    "    If best_params is given: skips Optuna, returns ITE_array only.\n",
    "    \"\"\"\n",
    "    # 1) hyperparam search\n",
    "    if provided_params is None:\n",
    "        import optuna\n",
    "        study = optuna.create_study(direction=\"minimize\",\n",
    "                                    study_name=f\"{model_name}_tune\")\n",
    "        def objective(trial):\n",
    "            # common\n",
    "            lr     = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "            wd     = trial.suggest_loguniform(\"wd\", 1e-5, 1e-2)\n",
    "            bs     = trial.suggest_categorical(\"bs\", [32, 128, 256])\n",
    "            epochs = trial.suggest_int(\"epochs\", 300, 800)\n",
    "\n",
    "            # model‚Äêspecific\n",
    "            if model_name == \"tarnet\":\n",
    "                rep1 = trial.suggest_int(\"rep1\", 20, 50 )\n",
    "                rep2 = trial.suggest_int(\"rep2\", 50, 100)\n",
    "                head = trial.suggest_int(\"head\", 50, 100)\n",
    "                drop = trial.suggest_uniform(\"drop\", 0.0, 0.001)\n",
    "                trainer = TARNetTrainer(X_train.shape[1], [rep1,rep2], [head], drop)\n",
    "\n",
    "            elif model_name == \"cfrnet\":\n",
    "                rep1   = trial.suggest_int(\"rep1\", 50, 200)\n",
    "                rep2   = trial.suggest_int(\"rep2\", 50, 200)\n",
    "                head   = trial.suggest_int(\"head\", 50, 200)\n",
    "                drop   = trial.suggest_uniform(\"drop\", 0.0, 0.001)\n",
    "                ipm_w  = trial.suggest_loguniform(\"ipm_weight\", 0.01, 10.0)\n",
    "                trainer = CFRNetTrainer(X_train.shape[1], [rep1,rep2], [head], drop, ipm_w)\n",
    "\n",
    "            elif model_name == \"cevae\":\n",
    "                ld = trial.suggest_int(\"latent_dim\", 10, 200)\n",
    "                hd = trial.suggest_int(\"hidden_dim\", 20, 400)\n",
    "                nl = trial.suggest_int(\"num_layers\", 2, 5)     # note: 2‚Üí5 to avoid pop error\n",
    "                ns = trial.suggest_categorical(\"num_samples\", [10,50,100,200])\n",
    "                trainer = CEVAETrainer(X_train.shape[1], ld, hd, nl, ns)\n",
    "\n",
    "            else:  # dragonnet\n",
    "                sh = trial.suggest_int(\"shared_hidden\", 50, 200)\n",
    "                oh = trial.suggest_int(\"outcome_hidden\", 50, 200)\n",
    "                trainer = DragonNetTrainer(X_train.shape[1], sh, oh)\n",
    "\n",
    "            return trainer.fit(\n",
    "                X_train, t_train, y_train,\n",
    "                X_val,   t_val,   y_val,\n",
    "                lr=lr, weight_decay=wd,\n",
    "                batch_size=bs, epochs=epochs\n",
    "            )\n",
    "\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        best_params = study.best_params\n",
    "        print(f\"üîç Best params for {model_name}: {best_params}\")\n",
    "    else:\n",
    "        best_params = provided_params\n",
    "    # 2) retrain on combined train+val\n",
    "    X_trn = np.vstack([X_train, X_val])\n",
    "    t_trn = np.concatenate([t_train, t_val])\n",
    "    y_trn = np.concatenate([y_train, y_val])\n",
    "\n",
    "    if model_name == \"tarnet\":\n",
    "        trainer = TARNetTrainer(\n",
    "            X_trn.shape[1],\n",
    "            [best_params['rep1'], best_params['rep2']],\n",
    "            [best_params['head']],\n",
    "            best_params['drop']\n",
    "        )\n",
    "    elif model_name == \"cfrnet\":\n",
    "        trainer = CFRNetTrainer(\n",
    "            X_trn.shape[1],\n",
    "            [best_params['rep1'], best_params['rep2']],\n",
    "            [best_params['head']],\n",
    "            best_params['drop'],\n",
    "            best_params['ipm_weight']\n",
    "        )\n",
    "    elif model_name == \"cevae\":\n",
    "        trainer = CEVAETrainer(\n",
    "            X_trn.shape[1],\n",
    "            best_params['latent_dim'],\n",
    "            best_params['hidden_dim'],\n",
    "            best_params['num_layers'],\n",
    "            best_params['num_samples']\n",
    "        )\n",
    "    else:\n",
    "        trainer = DragonNetTrainer(\n",
    "            X_trn.shape[1],\n",
    "            best_params['shared_hidden'],\n",
    "            best_params['outcome_hidden']\n",
    "        )\n",
    "\n",
    "    trainer.fit(\n",
    "        X_trn, t_trn, y_trn,\n",
    "        X_test, t_test, y_test,\n",
    "        lr=best_params['lr'],\n",
    "        weight_decay=best_params['wd'],\n",
    "        batch_size=best_params['bs'],\n",
    "        epochs=best_params['epochs']\n",
    "    )\n",
    "\n",
    "    if model_name == \"cevae\":\n",
    "        ite = trainer.predict(X_test)\n",
    "    else:\n",
    "        y0p, y1p = trainer.predict(X_test)\n",
    "        ite = y1p - y0p\n",
    "\n",
    "    return (ite, best_params) if provided_params is None else ite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b1e42",
   "metadata": {},
   "source": [
    "## Fitting synthetic data generated by causl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2a3a2",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89c5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments for strength_instr = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 2.3854,\tloss_y 1.6279, 1.6506, 0.0455,\tloss_eta 0.7576, 0.7917, 0.0683\n",
      "Warning: covariate dimension does not aligned with the specified input dimension; filling in the remaining dimension with noise.\n",
      "Warning: covariate dimension does not aligned with the specified input dimension; filling in the remaining dimension with noise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 07:30:03,303] A new study created in memory with name: tarnet_tune\n",
      "Exception ignored in: <function _xla_gc_callback at 0x351d55580>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
      "    def _xla_gc_callback(*args):\n",
      "    \n",
      "KeyboardInterrupt: \n",
      "[I 2025-04-26 07:30:08,798] Trial 0 finished with value: 0.7342360615730286 and parameters: {'lr': 0.004206814077835359, 'wd': 0.0014372372254375487, 'bs': 256, 'epochs': 716, 'rep1': 29, 'rep2': 93, 'head': 100, 'drop': 0.0007061397880518154}. Best is trial 0 with value: 0.7342360615730286.\n",
      "[I 2025-04-26 07:30:12,186] Trial 1 finished with value: 1.122741937637329 and parameters: {'lr': 1.2212100789764055e-05, 'wd': 0.0020118063827876172, 'bs': 128, 'epochs': 384, 'rep1': 42, 'rep2': 84, 'head': 55, 'drop': 0.0002811096668861708}. Best is trial 0 with value: 0.7342360615730286.\n"
     ]
    }
   ],
   "source": [
    "nrep = 30  # Number of repetitions\n",
    "n_tr = 1000  # Training sample size\n",
    "n_val = 400\n",
    "n_te = 400  # Testing sample size\n",
    "strength_instr_values = np.arange(1,4.5,1)  # Varying strength of instrumental variables\n",
    "nI = 4 # Fixed number of instrumental variables\n",
    "nX = 3\n",
    "nO = 3\n",
    "nS = 0\n",
    "binary_intervention = True\n",
    "num_iters = 800  # Fixed number of training iterations\n",
    "ate = 2\n",
    "strength_conf = 1\n",
    "strength_outcome = 1\n",
    "\n",
    "# Initialize tracker for strength_instr\n",
    "tracker = {strength_instr: {\"fr\": [], \"dr\": [], \"causalegm\":[], \"tarnet\":[], \"cfrnet\":[], \"cevae\":[], \"dragonnet\":[]}\n",
    "           for strength_instr in strength_instr_values}\n",
    "best_hps = {model: None for model in [\"tarnet\",\"cfrnet\",\"cevae\",\"dragonnet\"]}\n",
    "# Begin loop over strength_instr\n",
    "for strength_instr in strength_instr_values:\n",
    "    print(f\"Running experiments for strength_instr = {strength_instr}\")\n",
    "    p = nI + nX + nO + nS  # Update the number of covariates\n",
    "    \n",
    "    for rep in tqdm(range(nrep)):\n",
    "        # Generate training and testing data\n",
    "        df_tr = generate_data_causl(n=n_tr, nI=nI, nX=nX, nO=nO, nS=nS, ate=ate, \n",
    "                                    beta_cov=beta_cov, strength_instr=strength_instr, \n",
    "                                    strength_conf=strength_conf, \n",
    "                                    strength_outcome=strength_outcome, \n",
    "                                    binary_intervention=binary_intervention)\n",
    "        z_tr = torch.tensor(df_tr[[f\"X{i}\" for i in range(1, p + 1)]].values, dtype=torch.float32)\n",
    "        x_tr = torch.tensor(df_tr['A'].values, dtype=torch.int32).view(-1, 1) if binary_intervention else \\\n",
    "            torch.tensor(df_tr['A'].values, dtype=torch.float32).view(-1, 1)\n",
    "        y_tr = torch.tensor(df_tr['y'].values, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        z_tr_np = df_tr[[f\"X{i}\" for i in range(1, p + 1)]].values\n",
    "        x_tr_np = df_tr['A'].values\n",
    "        y_tr_np = df_tr['y'].values\n",
    "\n",
    "        df_val = generate_data_causl(n=n_val, nI=nI, nX=nX, nO=nO, nS=nS, ate=ate, \n",
    "                                    beta_cov=beta_cov, strength_instr=strength_instr, \n",
    "                                    strength_conf=strength_conf, \n",
    "                                    strength_outcome=strength_outcome, \n",
    "                                    binary_intervention=binary_intervention)\n",
    "\n",
    "\n",
    "        z_val_np = df_val[[f\"X{i}\" for i in range(1, p + 1)]].values\n",
    "        x_val_np = df_val['A'].values\n",
    "        y_val_np = df_val['y'].values\n",
    "\n",
    "        df_te = generate_data_causl(n=n_te, nI=nI, nX=nX, nO=nO, nS=nS, ate=ate, \n",
    "                                    beta_cov=beta_cov, strength_instr=strength_instr, \n",
    "                                    strength_conf=strength_conf, \n",
    "                                    strength_outcome=strength_outcome, \n",
    "                                    binary_intervention=binary_intervention)\n",
    "\n",
    "        z_te_np = df_te[[f\"X{i}\" for i in range(1, p + 1)]].values\n",
    "        x_te_np = df_te['A'].values\n",
    "        y_te_np = df_te['y'].values\n",
    "        z_te = torch.tensor(z_te_np, dtype=torch.float32)\n",
    "\n",
    "        model = Frengression(x_dim = x_tr.shape[1], y_dim = 1, z_dim =z_tr.shape[1], \n",
    "                             noise_dim=1, num_layer=3, hidden_dim=100, \n",
    "                             device=device, x_binary=binary_intervention, z_binary_dims=0)\n",
    "\n",
    "        # Train Frengression model\n",
    "        model.train_y(x=x_tr,\n",
    "                      z=z_tr, \n",
    "                      y=y_tr, \n",
    "                      num_iters=num_iters, lr=1e-4, print_every_iter=1000)\n",
    "\n",
    "        # Sample model distributions\n",
    "        P0 = model.sample_causal_margin(torch.tensor([0], dtype=torch.int32), sample_size=n_te).numpy().reshape(-1, 1)\n",
    "        P1 = model.sample_causal_margin(torch.tensor([1], dtype=torch.int32), sample_size=n_te).numpy().reshape(-1, 1)\n",
    "        ate_fr = np.mean(P1) - np.mean(P0)\n",
    "\n",
    "        # DR Estimation\n",
    "        ate_dr, _ = dr_ate(x_tr_np, y_tr_np, z_tr_np ,x_te_np, y_te_np, z_te_np)\n",
    "\n",
    "        for model in [\"tarnet\",\"cfrnet\",\"cevae\",\"dragonnet\"]:\n",
    "            if rep == 0:\n",
    "                ite, best_hps[model] = tune_and_eval(\n",
    "                    model,\n",
    "                    z_tr_np, x_tr_np, y_tr_np,\n",
    "                    z_val_np, x_val_np, y_val_np,\n",
    "                    z_te_np, x_te_np,y_te_np,\n",
    "                    provided_params=None,\n",
    "                    n_trials=20\n",
    "                )\n",
    "            else:\n",
    "                ite = tune_and_eval(\n",
    "                    model,\n",
    "                    z_tr_np, x_tr_np, y_tr_np,\n",
    "                    z_val_np, x_val_np, y_val_np,\n",
    "                    z_te_np, x_te_np,y_te_np,\n",
    "                    provided_params=best_hps[model]\n",
    "                )\n",
    "            tracker[strength_instr][model].append(ite.mean())\n",
    "\n",
    "\n",
    "        cegm_params = {'dataset': 'Semi_acic', \n",
    "                        'output_dir': '.', \n",
    "                        'v_dim': z_tr.shape[1], \n",
    "                        'z_dims': [1, 1, 1, 1], \n",
    "                        'lr': 0.0002, \n",
    "                        'alpha': 1, \n",
    "                        'beta': 1, \n",
    "                        'gamma': 10, \n",
    "                        'g_d_freq': 5, \n",
    "                        'g_units': [64, 64, 64, 64, 64], \n",
    "                        'e_units': [64, 64, 64, 64, 64], \n",
    "                        'f_units': [64, 32, 8], \n",
    "                        'h_units': [64, 32, 8], \n",
    "                        'dz_units': [64, 32, 8], \n",
    "                        'dv_units': [64, 32, 8], 'save_res': False, 'save_model': False, 'binary_treatment': True, 'use_z_rec': True, 'use_v_gan': True}\n",
    "        egm_model = cegm.CausalEGM(params=cegm_params, random_seed=42)\n",
    "        egm_model.train(data=[x_tr,y_tr,z_tr],n_iter=1000, verbose=False)\n",
    "        ate_causalegm=egm_model.getCATE(z_te).mean()\n",
    "\n",
    "        # Log results\n",
    "        tracker[strength_instr][\"fr\"].append(ate_fr)\n",
    "        tracker[strength_instr][\"dr\"].append(ate_dr)\n",
    "        tracker[strength_instr][\"causalegm\"].append(ate_causalegm)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_dir = \"result/binary\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tracker_serializable = {\n",
    "    str(k): [float(x) for x in v_dict.get(\"fr\",[])] \n",
    "              + []  # (we'll overwrite below) \n",
    "    for k, v_dict in tracker.items()\n",
    "}\n",
    "# actually build full dict:\n",
    "tracker_serializable = {\n",
    "    str(k): {\n",
    "        model: [float(x) for x in v_list]\n",
    "        for model, v_list in v_dict.items()\n",
    "    }\n",
    "    for k, v_dict in tracker.items()\n",
    "}\n",
    "\n",
    "# 3) write it out\n",
    "with open(os.path.join(output_dir, \"synthetic_1k.json\"), \"w\") as f:\n",
    "    json.dump(tracker_serializable, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bf788",
   "metadata": {},
   "source": [
    "## IHDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking trial = 0\n",
      "Running on valid trial = 0\n",
      "Epoch 1: loss 3.9422,\tloss_y 3.2075, 3.2219, 0.0289,\tloss_eta 0.7347, 0.7670, 0.0646\n",
      "Epoch 400: loss 1.0614,\tloss_y 0.4755, 0.9720, 0.9931,\tloss_eta 0.5860, 1.1017, 1.0315\n",
      "Epoch 800: loss 0.9473,\tloss_y 0.3557, 0.7377, 0.7640,\tloss_eta 0.5916, 1.0262, 0.8692\n",
      "Warning: covariate dimension does not aligned with the specified input dimension; filling in the remaining dimension with noise.\n",
      "Warning: covariate dimension does not aligned with the specified input dimension; filling in the remaining dimension with noise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 07:28:26,313] A new study created in memory with name: tarnet_tune\n",
      "[I 2025-04-26 07:28:28,582] Trial 0 finished with value: 0.05483689531683922 and parameters: {'lr': 0.006491187885625268, 'wd': 0.004936980702479022, 'bs': 256, 'epochs': 392, 'rep1': 36, 'rep2': 92, 'head': 99, 'drop': 0.0006358724407596506}. Best is trial 0 with value: 0.05483689531683922.\n",
      "[I 2025-04-26 07:28:32,857] Trial 1 finished with value: 3.3733069896698 and parameters: {'lr': 2.307907865982853e-05, 'wd': 0.0025256327336519034, 'bs': 32, 'epochs': 309, 'rep1': 20, 'rep2': 59, 'head': 92, 'drop': 0.0003527484933601588}. Best is trial 0 with value: 0.05483689531683922.\n",
      "[I 2025-04-26 07:28:39,931] Trial 2 finished with value: 0.25672170519828796 and parameters: {'lr': 0.00039374419049185164, 'wd': 0.0010826011185339763, 'bs': 32, 'epochs': 536, 'rep1': 22, 'rep2': 50, 'head': 71, 'drop': 0.0001100284911583589}. Best is trial 0 with value: 0.05483689531683922.\n",
      "[I 2025-04-26 07:28:41,594] Trial 3 finished with value: 7.220184803009033 and parameters: {'lr': 1.9957696194256604e-05, 'wd': 0.0014829904149539737, 'bs': 256, 'epochs': 325, 'rep1': 26, 'rep2': 62, 'head': 61, 'drop': 0.00014004495521865946}. Best is trial 0 with value: 0.05483689531683922.\n",
      "[I 2025-04-26 07:28:43,692] Trial 4 finished with value: 3.2993462085723877 and parameters: {'lr': 5.1840572504998654e-05, 'wd': 0.0029396768359838624, 'bs': 128, 'epochs': 312, 'rep1': 30, 'rep2': 63, 'head': 77, 'drop': 0.00010039602157550066}. Best is trial 0 with value: 0.05483689531683922.\n",
      "[W 2025-04-26 07:28:47,175] Trial 5 failed with parameters: {'lr': 2.872063814062349e-05, 'wd': 4.219579912116434e-05, 'bs': 32, 'epochs': 443, 'rep1': 46, 'rep2': 100, 'head': 70, 'drop': 0.0003309775339561526} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/34/4pny9pxj7z31glf0phhx3b280000gn/T/ipykernel_18067/3014561015.py\", line 53, in objective\n",
      "    return trainer.fit(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/linyingyang/Documents/Project/frengression/frengression/paper_exp/models.py\", line 79, in fit\n",
      "    optimizer.step()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py\", line 430, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-26 07:28:47,178] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarnet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfrnet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcevae\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdragonnet\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_trials \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         ite, best_hps[model] \u001b[38;5;241m=\u001b[39m tune_and_eval(\n\u001b[1;32m     67\u001b[0m             model,\n\u001b[1;32m     68\u001b[0m             z_tr_np, x_tr_np, y_tr_np,\n\u001b[1;32m     69\u001b[0m             z_tr_np, x_tr_np, y_tr_np,\n\u001b[1;32m     70\u001b[0m             z_te_np, x_te_np,y_te_np,\n\u001b[1;32m     71\u001b[0m             provided_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         ite \u001b[38;5;241m=\u001b[39m tune_and_eval(\n\u001b[1;32m     76\u001b[0m             model,\n\u001b[1;32m     77\u001b[0m             z_tr_np, x_tr_np, y_tr_np,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m             provided_params\u001b[38;5;241m=\u001b[39mbest_hps[model]\n\u001b[1;32m     81\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[28], line 60\u001b[0m, in \u001b[0;36mtune_and_eval\u001b[0;34m(model_name, X_train, t_train, y_train, X_val, t_val, y_val, X_test, t_test, y_test, provided_params, n_trials)\u001b[0m\n\u001b[1;32m     51\u001b[0m         trainer \u001b[38;5;241m=\u001b[39m DragonNetTrainer(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], sh, oh)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     54\u001b[0m         X_train, t_train, y_train,\n\u001b[1;32m     55\u001b[0m         X_val,   t_val,   y_val,\n\u001b[1;32m     56\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd,\n\u001b[1;32m     57\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbs, epochs\u001b[38;5;241m=\u001b[39mepochs\n\u001b[1;32m     58\u001b[0m     )\n\u001b[0;32m---> 60\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n\u001b[1;32m     61\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Best params for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[28], line 53\u001b[0m, in \u001b[0;36mtune_and_eval.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     50\u001b[0m     oh \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome_hidden\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     51\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m DragonNetTrainer(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], sh, oh)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     54\u001b[0m     X_train, t_train, y_train,\n\u001b[1;32m     55\u001b[0m     X_val,   t_val,   y_val,\n\u001b[1;32m     56\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mwd,\n\u001b[1;32m     57\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbs, epochs\u001b[38;5;241m=\u001b[39mepochs\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Project/frengression/frengression/paper_exp/models.py:79\u001b[0m, in \u001b[0;36mTARNetTrainer.fit\u001b[0;34m(self, X_train, t_train, y_train, X_val, t_val, y_val, lr, weight_decay, batch_size, epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, yb)\n\u001b[1;32m     78\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     adam(\n\u001b[1;32m    224\u001b[0m         params_with_grad,\n\u001b[1;32m    225\u001b[0m         grads,\n\u001b[1;32m    226\u001b[0m         exp_avgs,\n\u001b[1;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    229\u001b[0m         state_steps,\n\u001b[1;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m func(\n\u001b[1;32m    785\u001b[0m     params,\n\u001b[1;32m    786\u001b[0m     grads,\n\u001b[1;32m    787\u001b[0m     exp_avgs,\n\u001b[1;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    790\u001b[0m     state_steps,\n\u001b[1;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    803\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "binary_intervention = True\n",
    "num_iters = 1000\n",
    "p = 25\n",
    "z_binary_dims = 19\n",
    "path = '/Users/linyingyang/Documents/Project/frengression/frengression/data_causl'\n",
    "\n",
    "# Initialize tracker for valid trials\n",
    "\n",
    "valid_trials = 0\n",
    "max_trials = 100  # We want results from 100 valid trials\n",
    "trial = 0\n",
    "\n",
    "\n",
    "tracker = {\"fr\": [], \"dr\": [], \"causalegm\":[], \"tarnet\":[], \"cfrnet\":[], \"cevae\":[], \"dragonnet\":[]}\n",
    "best_hps = {model: None for model in [\"tarnet\",\"cfrnet\",\"cevae\",\"dragonnet\"]}\n",
    "\n",
    "while valid_trials < max_trials:\n",
    "    print(f\"Checking trial = {trial}\")\n",
    "    df_tr, df_te = process_data(path=path, trial=trial)\n",
    "\n",
    "    # Skip this trial if any y_factual in df_tr exceeds 20\n",
    "    if (df_tr['y_factual'] > 20).any():\n",
    "        print(f\"Skipping trial {trial} because y_factual > 20\")\n",
    "        trial += 1\n",
    "        continue\n",
    "\n",
    "    print(f\"Running on valid trial = {trial}\")\n",
    "    \n",
    "    # Prepare tensors for training\n",
    "    z_tr = torch.tensor(df_tr[[f\"X{i}\" for i in range(1, p + 1)]].values, dtype=torch.float32)\n",
    "    x_tr = torch.tensor(df_tr['treatment'].values, dtype=torch.float32).view(-1, 1)\n",
    "    y_tr = torch.tensor(df_tr['y_factual'].values, dtype=torch.float32).view(-1, 1)\n",
    "    ate_sample = torch.tensor(np.mean(df_tr['mu1'].values - df_tr['mu0'].values), dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    # DR ATE estimation\n",
    "    z_tr_np = df_tr[[f\"X{i}\" for i in range(1, p + 1)]].values\n",
    "    x_tr_np = df_tr['treatment'].values\n",
    "    y_tr_np = df_tr['y_factual'].values\n",
    "\n",
    "    z_te_np = df_te[[f\"X{i}\" for i in range(1, p + 1)]].values\n",
    "    x_te_np = df_te['treatment'].values\n",
    "    y_te_np = df_te['y_factual'].values\n",
    "    hat_dr, _ = dr_ate(x_tr_np, y_tr_np, z_tr_np, x_te_np, y_te_np, z_te_np)\n",
    "\n",
    "    # Initialize Frengression model\n",
    "    model = Frengression(x_tr.shape[1], y_tr.shape[1], z_tr.shape[1], \n",
    "                         noise_dim=1, num_layer=3, hidden_dim=400, \n",
    "                         device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "                         x_binary=binary_intervention, z_binary_dims=19)\n",
    "    \n",
    "\n",
    "    model.train_y(x_tr, z_tr, y_tr, num_iters=num_iters, lr=1e-4, print_every_iter=400)\n",
    "\n",
    "\n",
    "    # Sample model distributions\n",
    "    P0 = model.sample_causal_margin(torch.tensor([0], dtype=torch.int32), sample_size=n_te).numpy().reshape(-1, 1)\n",
    "    P1 = model.sample_causal_margin(torch.tensor([1], dtype=torch.int32), sample_size=n_te).numpy().reshape(-1, 1)\n",
    "    ate_fr = np.mean(P1) - np.mean(P0)\n",
    "\n",
    "    # DR Estimation\n",
    "    ate_dr, _ = dr_ate(x_tr_np, y_tr_np, z_tr_np ,x_te_np, y_te_np, z_te_np)\n",
    "\n",
    "    for model in [\"tarnet\",\"cfrnet\",\"cevae\",\"dragonnet\"]:\n",
    "        if valid_trials == 0:\n",
    "            ite, best_hps[model] = tune_and_eval(\n",
    "                model,\n",
    "                z_tr_np, x_tr_np, y_tr_np,\n",
    "                z_tr_np, x_tr_np, y_tr_np,\n",
    "                z_te_np, x_te_np,y_te_np,\n",
    "                provided_params=None,\n",
    "                n_trials=20\n",
    "            )\n",
    "        else:\n",
    "            ite = tune_and_eval(\n",
    "                model,\n",
    "                z_tr_np, x_tr_np, y_tr_np,\n",
    "                z_tr_np, x_tr_np, y_tr_np,\n",
    "                z_te_np, x_te_np,y_te_np,\n",
    "                provided_params=best_hps[model]\n",
    "            )\n",
    "        tracker[model].append(ite.mean())\n",
    "\n",
    "\n",
    "    cegm_params = {'dataset': 'Semi_acic', \n",
    "                    'output_dir': '.', \n",
    "                    'v_dim': z_tr.shape[1], \n",
    "                    'z_dims': [1, 1, 1, 1], \n",
    "                    'lr': 0.0002, \n",
    "                    'alpha': 1, \n",
    "                    'beta': 1, \n",
    "                    'gamma': 10, \n",
    "                    'g_d_freq': 5, \n",
    "                    'g_units': [64, 64, 64, 64, 64], \n",
    "                    'e_units': [64, 64, 64, 64, 64], \n",
    "                    'f_units': [64, 32, 8], \n",
    "                    'h_units': [64, 32, 8], \n",
    "                    'dz_units': [64, 32, 8], \n",
    "                    'dv_units': [64, 32, 8], 'save_res': False, 'save_model': False, 'binary_treatment': True, 'use_z_rec': True, 'use_v_gan': True}\n",
    "    egm_model = cegm.CausalEGM(params=cegm_params, random_seed=42)\n",
    "    egm_model.train(data=[x_tr,y_tr,z_tr],n_iter=1000, verbose=False)\n",
    "    ate_causalegm=egm_model.getCATE(z_te).mean()\n",
    "    tracker['causalegm'].append(ate_causalegm)\n",
    "\n",
    "    # Increment valid trials counter and move to the next trial\n",
    "    valid_trials += 1\n",
    "    trial += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda8d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.996327"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"result/binary\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tracker_serializable = {\n",
    "    str(k): [float(x) for x in v_dict.get(\"fr\",[])] \n",
    "              + []  # (we'll overwrite below) \n",
    "    for k, v_dict in tracker.items()\n",
    "}\n",
    "# actually build full dict:\n",
    "tracker_serializable = {\n",
    "    str(k): {\n",
    "        model: [float(x) for x in v_list]\n",
    "        for model, v_list in v_dict.items()\n",
    "    }\n",
    "    for k, v_dict in tracker.items()\n",
    "}\n",
    "\n",
    "# 3) write it out\n",
    "with open(os.path.join(output_dir, \"ihdp.json\"), \"w\") as f:\n",
    "    json.dump(tracker_serializable, f, indent=4)ate_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db703bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
